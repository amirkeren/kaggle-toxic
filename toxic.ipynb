{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import regularizers\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download from https://www.kaggle.com/c/8076/download/train.csv.zip\n",
    "train = pd.read_csv('data/train.csv')[:100]\n",
    "train_sentences = train.comment_text.fillna('FILLNA').values\n",
    "\n",
    "# Download from https://www.kaggle.com/c/8076/download/test.csv.zip\n",
    "test = pd.read_csv('data/test.csv')[:100]\n",
    "test_sentences = test.comment_text.fillna('FILLNA').values\n",
    "\n",
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(np.concatenate((train_sentences, test_sentences), axis=0))\n",
    "train_tokenized = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_tokenized = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "maxlen = max(max(len(l) for l in train_tokenized), max(len(l) for l in test_tokenized))\n",
    "\n",
    "X_train = sequence.pad_sequences(train_tokenized, maxlen=maxlen)\n",
    "y_train = train[CLASSES].values\n",
    "X_test = sequence.pad_sequences(test_tokenized, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(Counter([token for sublist in train_tokenized + test_tokenized for token in sublist])) + 1\n",
    "\n",
    "hyper_params = {\n",
    "    'l1_regularization': 0.01,\n",
    "    'l2_regularization': 0.01,\n",
    "    'validation_split': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10,\n",
    "    'embedding_size': 128,\n",
    "    'keep_probability': 0.9,\n",
    "    'lstm_size': 50,\n",
    "    'dense_size': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, hyper_params['embedding_size']))\n",
    "model.add(LSTM(hyper_params['lstm_size'], return_sequences=True))\n",
    "model.add(LSTM(hyper_params['lstm_size']))\n",
    "model.add(Dropout(1 - hyper_params['keep_probability']))\n",
    "model.add(Dense(hyper_params['dense_size'], activation='relu', \n",
    "                kernel_regularizer=regularizers.l2(hyper_params['l2_regularization']), \n",
    "                activity_regularizer=regularizers.l1(hyper_params['l1_regularization'])))\n",
    "model.add(Dropout(1 - hyper_params['keep_probability']))\n",
    "model.add(Dense(len(CLASSES), activation='sigmoid', \n",
    "                kernel_regularizer=regularizers.l2(hyper_params['l2_regularization']), \n",
    "                activity_regularizer=regularizers.l1(hyper_params['l1_regularization'])))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - 5s 61ms/step - loss: 2.2091 - acc: 0.7926 - val_loss: 1.5688 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 3s 36ms/step - loss: 2.1699 - acc: 0.9389 - val_loss: 1.5456 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 3s 35ms/step - loss: 2.1453 - acc: 0.9389 - val_loss: 1.5239 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 3s 34ms/step - loss: 2.1230 - acc: 0.9389 - val_loss: 1.5027 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 3s 36ms/step - loss: 2.1012 - acc: 0.9389 - val_loss: 1.4821 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 3s 35ms/step - loss: 2.0801 - acc: 0.9389 - val_loss: 1.4621 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 3s 38ms/step - loss: 2.0595 - acc: 0.9389 - val_loss: 1.4427 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 3s 38ms/step - loss: 2.0395 - acc: 0.9389 - val_loss: 1.4238 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 3s 35ms/step - loss: 2.0200 - acc: 0.9389 - val_loss: 1.4053 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 3s 34ms/step - loss: 2.0010 - acc: 0.9389 - val_loss: 1.3875 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=hyper_params['batch_size'], epochs=hyper_params['epochs'], \n",
    "                    validation_split=hyper_params['validation_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download from https://www.kaggle.com/c/8076/download/sample_submission.csv.zip\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')[:100]\n",
    "sample_submission[CLASSES] = model.predict(X_test)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
